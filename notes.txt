TOPICS
1. IMPORTANCE
2. MEASURING
3. TESTS AND TOOLS
4. SETTING GOALS
5. IMPROVING


1. IMPORTANCE:
Three reasons for web performance:

- User experience;
- SEO;
- Online Adverrtising.

"For effective communication, some response is needed within two seconds of a request.
A wait longer than two secondes breaks concentration and affects productivity."

1.1 USERS EXPECTATIONS
- A user feels a response is instant at .01 seconds;
- A user experiences uninterrupted flor at 1 second;
- Users break flow and feel frustration at 10 seconds.

- 40% os users abandon a site at 3 seconds;
- 75% os users that experience a "slow" site will not return.


1.2 SEO
"Search ranking change that incorporates page experience metrics. We will introduce a new
signal that combines Core Web Vitals with our existing signals."

"You need to be fast to rank well."


1.3 ONLINE ADVERTISING
"Page speed is a ranking factor for Google Ads. A slow page can lead to a lower Quality Score,
which can increase your cost per click and reduce your ad position."

Bounce rate and performance are directly related for a lot of websites.

"65% performance improvement reduced bounce rate 20% and 200% time on page."

"100ms improvement in page load time resulted in 1% increase in revenue per visitor."
"1 second improvement in page load time resulted in 27% increase in conversion rate."


2. MEASURING:
- Legacy metrics;
- Core Web Vitals;
- More metrics;
- Capturing metrics;
- Browser support.


Waterfall Charts:
- A waterfall chart is a visual representation of the loading process of a web page.
- It shows the time taken for each resource to load and how they are related to each other.
- It helps identify bottlenecks and optimize loading times.

Flame Charts:
- It's a different dimension of the waterfall chart.
- It's to measure time and is measured in milliseconds and microseconds.
- The size of the bars indicates how much of the parent task was the child task consuming.

GRAY: Browser Tasks
BLUE: Parse HTML
PURPLE: Layout and Paint
DARK YELLOW: Evaluate and Compile Scripts (passthrough) - TOP LEVEL OF JAVASCRIPT
LIGHT YELLOW: Javascript Execution (working)
GREEN: Extensions


Why are Flame Charts important in web performance?
We have to talk about the main thread on the browser, the single thread of work the browser has for handling user events,
layout, paint, and running Javascript.

Everything is operating in a single thread called the main thread.

So if you write a bunch of Javascript that is really slow and does a whole bunch of work, it can stop other things
from happening, other things like firing events, laying out the document and painting things.

It can stop things from happening that the user would expect to happen.

2.1 LEGACY METRICS

- DOMContentLoaded:
The HTML downloaded and deferred scripts have executed. Images might not have loaded but
the HTML document itself is done.

- Load:
The page is fully loaded, including all images and other resources (except those that
are lazy-loaded).

THE PROBLEM WITH THE LEGACY METRICS
- They are not user-centric;
- They are not real-world metrics;

The beginning of the client-side rendering single-page applications change the structure
of the web pages. The legacy metrics are not enough to measure the performance of these
pages.

After it, Google started to measure the performance of the web pages using the Core Web Vitals.


2.2 CORE WEB VITALS

The core web vitals measure three things, three specific kinds of performance:
1. How fast your site visibly loads;
2. How smooth things load;
3. How quickly users can interact.

Those are measured with three different metrics called:
- Largest Contentful Paint (LCP);
- Cumulative Layout Shift (CLS);
- Interaction to Next Paint (INP).

"Search ranking incorporates the [CORE WEB VITALS] page experience metrics."


2.2.1 LARGEST CONTENTFUL PAINT (LCP)
How fast your site visibly loads the most important element but instead they measure the largest;

What is the Largest?
- <img>;
- <video>;
- css:background-image;
- Text-elements;

There are some rules to define what is the Largest:
- Opacity > 0;
- Size < 100% (if is 100% it's probably a background image);
- Low Entropy Images < 0.05.

What is Entropy?
Entropy is calculated by the number of bits per visible pixel shown.

Ex:
One image with 31,101,368 bytes rendering at full size as 2800 x 1200 withe 3.360,000 pixels.
The Entropy is calculated by simple dividing the number of the bits for the number of the pixels, in the example
we will have an entropy of 9.39.

Considerations:
LCP measurement stops after the first user interaction.

How fast it needs to be?
To be classified as 'Good', the content needs to render at 2.5sec. If is between 2.5sec and 4.0 sec it is classified
as 'Needs Improvement', after 4.0sec is considered 'Poor'.

- 0 to 2.5sec = GOOD;
- 2.5sec to 4.0sec = NEEDS IMPROVEMENT;
- after 4.0sec = POOR.

- Hovering are not considering as a User Interaction, only clicks;


2.2.2 CUMULATIVE LAYOUT SHIFT (CLS)
CLS measure how smooth and predictably elements load into the page, because if the user feels like is predictable
they can start to interact sooner.

A layout shift is measured by the impact fraction. How much of the page was impacted by the move, times the distance fraction,
so how much across the viewport did it move. It's only concerned with the viewport.

How much of that viewport was impacted by the shift and how far across the viewport did it move.

Layout shift happens in both dimensions, width and height.

Layout shift happens in every device, Mobile, Desktop, Tablet, etc, and Google cares about all of them.

Example:
- Viewport height of 768px (In the example it only shifts down).
- The impact size is the whole thing, and in the example is 708px;
- The impact fraction is calculated byt the impact size divided by the height: 708 / 768 = 0.922;

92.2% of the viewport was impacted by this layout shift.

Distance moved is 180px, so the Distance Fraction is 180 / 768px = 0.234 or 23%;

So, the score of the layout is the multiplication of those two things together, so 0.922 * 0.234 = 0.215;

The cumulative means the sum of all layout shifts. It should be 1, 2,3 or more layout shifts.

If the layout shift happens in less than 500ms for user actions it doesn't count.

The classification is 0.1sec as GOOD, between 0.1sec to 0.25sec as NEEDS IMPROVEMENT and after 0.25sec as POOR;
- 0.1sec = GOOD;
- 0.1sec to 0.25sec = NEEDS IMPROVEMENT;
- after 0.25sec = POOR.

The use of skeletons should prevent layout shifts, because the space is reserved and when you swap it out the
layout does not move around it.


2.2.3 INTERACTION TO THE NEXT PAINT (INP)
How long does it take between an interaction and the next the browser can paint.

How quickly users can interact.

If the user have no interaction you will not have an INP score.

What is an interaction?
- Click;
- Drag;
- Touch;
- Keypress.

BUT NOT:
- Scroll.

When you click in a button, the INP is measuring the time from that click and the update of the clicked element.

The fundamental tactic to have a good measurement is to paint sooner, and we can do less.

Another way is to user asynchronous calls and functions.

INP is not measure a single time it happens its measuring every time it happens, it's noty adding them together, but
it is picking the worst one.

Every time the user interact it generates an INP.

CONSIDERATIONS
- There might not be an interaction;
- We don't know the worst until it's over;
- Heavily influenced by device capability.

The classification is 0 to 200ms as GOOD, between 200ms and 500ms as NEEDS IMPROVEMENT and after 500ms as POOR;
- 0 to 200ms = GOOD;
- 200ms to 500ms = NEEDS IMPROVEMENT;
- after 500ms = POOR.

That's the importance of using Spinners or Loading messages between calls and interactions, because the spinners and
the loading messages are considered as a painting.


2.3 MORE METRICS

2.3.1 TIME TO FIRST BYTE
TTFB is the time it takes for the browser to receive the first byte of data from the server after making a request.

It is measuring your server and network performance.

START TIME > REDIRECTED > SERVICE WORKER INIT > SERVICE WORKER FETCH EVENT> HTTP CACHE > DNS > TCP > REQUEST >
EARLY HINTS > RESPONSE ||   TTFB   || PROCESSING > LOAD

In TTFB if the time is less than 800ms is considered as GOOD, if is between 800ms and 1800ms is NEEDS IMPROVEMENT and
after 1800ms is POOR;

- 0 to 800ms = GOOD;
- 800ms to 1800ms = NEEDS IMPROVEMENT;
- after 1800ms = POOR.

TTFB affects your LCP, because if the TTFB is slow the LCP will be slow too.


2.3.2 FIRST CONTENTFUL PAINT
It is a kind of LCP, but is the First Content instead of the Largest Content.

Is about how fast your site visibly loads the something.

The FCP is the first time the browser renders any content on the page, including text, images, and other elements.

To be considered as GOOD the FCP needs to be less than 1.8sec, between 1.8sec and 3.0sec is NEEDS IMPROVEMENT and after 3.0sec is POOR;

- 0 to 1.8sec = GOOD;
- 1.8sec to 3.0sec = NEEDS IMPROVEMENT;
- after 3.0sec = POOR.

FLAME CHART EXAMPLE
|--------------------- LCP ---------------------|
|------- FCP --------|
|-- TTFB --|


2.4 CAPTURING METRICS

2.4.1 PERFORMANCE API
The performance API and the Performance Observer API are the two APIs that are used to capture the metrics.

The performance API have some key things that are interesting:
- performance.now() - gives you the time in milliseconds since the page started loading, it is the high-resolution
timestamp relative to start of page;

- performance.mark() - allows you to create a mark in the performance timeline;

- performance.measure() - allows you to measure the time between two marks;

- performance.getEntriesByName() - allows you to get the entries by name;

- performance.getEntriesByType() - allows you to get the entries by type;

- performance.getEntries() - allows you to get all the entries, like Page Navigation, Resource Request, Performance
 Events, Custom Events, etc.;

- performance.clearMarks() - allows you to clear the marks;

- performance.clearMeasures() - allows you to clear the measures;

- performance.getEntriesByType('mark') - allows you to get the entries by type 'mark';


2.4.2 PERFORMANCE OBSERVER API
The problem of the Performance API is that it is not real-time, it is a snapshot of the performance and if you start
to interact a lot with those metrics you probably will start to go down the performance just to get the metrics.

The Performance Observer API allows you to observe the performance of the page in real-time and get the metrics as they
happen.

The Performance Observer API is a way to get the performance metrics in real-time and it is a way to get the metrics
that are not available in the Performance API.

- performanceObserver.observe() - allows you to observe the performance of the page in real-time;
- performanceObserver.takeRecords() - allows you to take the records of the performance;
- performanceObserver.disconnect() - allows you to disconnect the observer;
- performanceObserver.getEntries() - allows you to get the entries of the performance;

2.4.3 WEB-VITALS.JS
Is a package from Google that allows you to get the Core Web Vitals metrics in real-time, and it is a way to get the
metrics that are not available in the Performance API.

The web-vitals.js allows you to subscribe to some particular events and handle all of the other idiosyncrasies about
adding things together.

Its like:
"import { onLCP, onCLS, onINP } from 'web-vitals';

onLCP(console.log);
onCLS(console.log);
onINP(console.log);"

2.4.4 PERFORMANCE METRIC BROWSER SUPPORT
If you want to use these metrics you will not measure Safari at all, because WEBKIT doesn't support the core web vitals
and GECKO only supports the LCP but not the CLS and INP.

- BLINK: Chrome, Edge, Opera, Samsung, Brave and Arc.
- WEBKIT: Safari, Mobile Safari and Chrome on IOS.
- GECKO: Firefox.

3 TESTS AND TOOLS

- Testing Methods;
- Common Tools;
- Real User Monitoring.

3.1 TESTING PERFORMANCE METHODS

Where do we measure from?
Fundamentally, what we are doing is we have a host somewhere, it could be a server, it could be your cloud, it could be
the hosting service, and then you're delivering that content across the Internet to your users and rendering it on their
device. There's a couple of places that we can measure web performance from, the most common place is the Lab Data, when
they create a test environment and run the test in a controlled environment, and the other is the Field Data, when they
measure the performance of the page in the real world, with real users and real devices.

The problem is any of this devices will measure what the users are really experiencing, because the users are
using different devices, different networks, different browsers and different locations.


3.1.1 STATISTICS

3.1.1.1 AVERAGE PROBLEMS
Average is the go-to tool that most people will use when they want to look at a lot of data. The measurement of the
average users performances never will be the same as the median users performance, because the average is influenced
by the outliers, and the median is not. The average is a good tool to look at the data, but it is not a good tool to
look at the performance of the users, because the average is not a good representation of the user performance.

For this reason when we talk about performance data we don't talk about averages we talk about percentiles

3.1.1.2 PERCENTILES
The percentiles iw the halfway of the user metrics, between the best performance data and the worst performance data.

The 50th percentile is the median, and the 95th percentile is the 95% of the users that are performing better than
the 95% of the users that are performing worse. The 95th percentile is the most common metric that we use to measure
the performance of the users, because it is the most common metric that we use to measure the performance of the
users.

And then we conclude: LAB DATA IS EASIER AND FIELD DATA IS MORE ACCURATE. LAB DATA IS DIAGNOSTIC AND FIELD DATA IS
EXPERIENCE.


3.2 COMMON TOOLS

3.2.1 LIGHTHOUSE
Lighthouse is a tool that is used to measure the performance of the page in a controlled environment.

To have a good measurement, the first thing to do in Lighthouse is to turn on the responsive mode. After ir we can
choose the decive of the kind of user that we want to test, some good options is: Iphone 12 PRO or Small laptop, but
you can create one for yourself or edit the existing devices.

After, we can choose how we are connected to the internet, we can choose between 3G, 4G, 5G, WiFi or Bad Connection.

And in the performance tab we can select configurations and in the CPU tab we can slow down the CPU to 4x, 6x or 20x. To
select 6x could be a good option to test the performance of the page in a real world scenario.

After these configurations we can run to Analyze the page and see the results.

3.2.1.1 ANALYZING RESULTS IN LIGHTHOUSE
- Performance Score: The performance score is a number between 0 and 100 that represents the performance of the page.
- Performance Metrics: The performance metrics are the metrics that are used to measure the performance of the page.
- Performance Audits: The performance audits are the audits that are used to measure the performance of the page.


3.2.2 WEB VITALS EXTENSION
The web vitals extension is a Chrome extension that allows you to measure the performance of the page in real-time,
and it is a way to get the metrics that are not available in the Performance API.


3.2.3 CHROME USER EXPERIENCE REPORT
The Chrome User Experience Report is how Google knows how your performance is,  so it can decide wether or not it wants
to penalize your ranking or not.

A Blink browser is capable to capture the Core Web Vitals.

3.2.4 SPEED CHECK
The speed check is a tool that allows you to measure the performance of the page in real-time, and it is a way to
get the metrics that are not available in the Performance API.

You can check, in the tool,the web performance of the site that you want, showing all the core web vitals report and
evaluation.

3.2.5 PAGESPEED INSIGHTS

pagespeed.web.dev

Its basic Lighthouse but running as a synthetic test.

3.2.6 WEBPAGE TEST

webpagetest.org

This is a full report engine, you can check a lot of metrics of any website that you want.


3.3 REAL USER MONITORING
Real User Monitoring (RUM) is a data that you can run to collect that data from those real users yourself.

There's a comparison between CRuX vs Real User Monitoring

CRuX
- Field Data;
- Logged in Chrome Users;
- Top 1M Public Websites;
- Anonymous and Public;
- 28 Day Rolling Average;
- Google Big Query.

Real User Monitoring
- Field Data;
- All Users;
- Private Sites;
- Private Details;
- Realtime;
- Custom Dashboard and Alerts.


3.3.1 RUM TOOLS
The difference between then is about the project that you are developing, if you are ina big enterpise place or have a
big website you can go to some Enterprise Rum, like:
- Akamai mPulse (This is the best choice if money is not the problem);
- Dynatrace;
- AppDynamics;
- DataDog;
- Sentry.

If you are in a simple project and wants to monitor only your own project you can use some Project RUM, like:
- Request Metrics;
- SpeedCurve;
- RUMVision;
- Pingdom;
- Raygun.

Just about REQUEST METRICS:

requestmetrics.com

- Real Time Data;
- Filtering Views;
- User Information;
- Watefall;
- Core Web Vital Attribution;
- CrUx Integration;
- Resource Reports;

Its totally customizable and offers to the developer a lot of usefull and trustful information about the performance
of the page.


4. SETTING GOALS

- How fast is enough?
- Who gets to decide;
- Understanding users;


4 .1 HOW FAST IS ENOUGH?
Users tend to remember thing slower than how they actually happened.

The Psychology of Waiting
1 - People want to start;
2 - Bored waits feel slower;
3 - Anxious waits feel slower
4 - Unexplained waits feel slower;
5 - Uncertain waits feel slower;
6 - People will wait for value.

The people not only wait for value, but they want to wait longer if something is perceived as high value.


4.2 WHO GETS TO DECIDE?
The short answer is NOT YOU.

There's three people, three concepts that get to decide what's fast enough:

- User Experience;
- Competitors;
- SEO Page Rank.

For this, you only need to thought about it: What do your users tell you, What do your competitors tell you and
what does SEO tell you.

4.2.1 USER EXPERIENCE
If you look at user experience, to understand what user experience is telling you about web performance you have to look
at your business metrics, not your web performance metrics. So what your business metrics are will depend on your unique
situation, but it could be things like:

- Bounce Rate;
- Session Time;
- Add-to-Cart Rate;
- Cart Abandonment Rate;
- Conversion Rate.

Follow your Business Metrics. You have to look at your metrics and how your business metrics and performance metrics
relate. Because your users will be value different things and will be affected at different thresholds.

4.2.2 COMPETITORS
For a user to notice the difference between you and them, generally, you need to be 20% faster. The Weber's Law, the
rule of the 20% is a good rule of thumb to follow.
Target is just 4% faster than Walmart, but the users don't notice and in reality, nobody cares. User's not notice 4%
difference.

Instead, Target is 57% faster than KMart, and the results of the performance ranting and conversion rate are noticed,
the users feel the difference and see that as a clearly faster site.

4.2.3 SEO PAGE RANK
You need to have a good measurement over the Core Web Vitals and the SEO Page Rank, because if you don't have a good
rating you probably will be penalized on your page rank.


4.3 UNDERSTANDING USERS

Who are these users?

First of all, are they users of Mobile Devices or Desktop Devices? or better, What is the size of the screen that they
are using?

What you see in your workstation or in your developer laptop, it's probably nothing like what your real users do.

Other thing is about the Operational System, are they users of Android? IOS? Windows? Linux?

So the average user on the internet is nowhere close to what we typically of and what we tipically have in our pockets
as developers.

Network speed is another issue, so if you're trying to base maybe your chrome speed tests on, these would be good
numbers to base your throttles based on how you should things down.

The future is not evenly distributed yet, because those numbers are wildly different depending on where in the world
you go. Some places, connection speeds is 250 megabits or better and other places crawl along at less than 10.

You should understand your users, using an analytics tool or real user monitoring tool, you should understand who your
users are, like what operating system, browser, device, network speed, what country, etc. and then you can set your
goals based on that.



5 IMPROVING

The first of all, focus on the easiest fixes for your worst metric from your real user data. If you're not basing this
off of real user data, you don't really know what you are working on. If you are basing your measurement only in
Lighthouse score,  you don't know if that's real, you don't know what that Lighthouse score was represents reality or
the real users.

So base it off of real data from CrUx or from your ram tool, and then look at whichever metric that's interesting to
you is the worst one, because if you're just starting, there might be more than one that's bad, but focus on the worst
one, and then do the easiest thing for your structure first because you don't need to do everything.

We will see:

- Improving TTFB;
- Improving FCP;
- Improving LCP;
- Improving Returning Experience;
- Improving CLS;
- Improving INP;


5.1 IMPROVING TTFB

Time To First Byte is how quickly your host responds to the request and TTFB doesn't exist in isolation, it is part of
FCP, which is part of LCP. Even if you  are trying to make your LCP faster and the TTFB are slow, that's a big
contributing factor, and it's gonna make everything else faster along the way, and it's gonna make everything else
faster along the way.


5.1.1 TTFB IMPROVEMENT TACTICS

1. Compress HTTP Responses;
2. Efficient Protocols;
3. Host Capacity;
4. Host Proximity.


5.1.1.1 COMPRESS HTTP RESPONSES

Reduce the size of plain text HTML, CSS, Javascript. The way to do that is with a couple of protocols called Gzip and
Brotli.

In the example, the compressing by Gzip made the document with 25% of the original size and in Brotli was 1.7% of the
original file.


5.1.1.2 EFFICIENT PROTOCOLS

HTTP 1.1 was the first protocol, HTTP 2.0 was an improvement in its protocol and have improved the number of
requisitions over client and server. Now we have the HTTP 3.0, this protocol don't use a TCP Protocol, instead of it,
he uses a protocol named QUIC, which is a UDP Protocol.

The difference between an TCP Protocol and a UDP Protocol is that TCP request on the right-hand side is guaranteed, so
if you are talking over TCP, I send you something, and you acknowledge that you got it by saying ACK, and then I send
something else, and then you say, ACK... There's a redundant step of communication that happens for every single request
to acknowledge that the other party received the information that I just sent, over and over and over. Whereas the UDP
doesn't really give a crap if the other person got it. e simply throws away what the user request, it's a little bit
more faulty in theory, but it's way faster, because you don't have to sit there and wait to make sure the other party
got it.

The HTTP 3.0 Drawbacks is:
- Require HTTPS ( so does HTTP/2 );
- UDP Networking;
- Difficult to Debug (curl);


5.1.1.3 HOST CAPACITY

You need to right-sizing your host for your workload, making sure that you've given the appropriate amount of capacity
to your service so that it can turn things on fast. You need to remove the artificial delay.


5.1.1.4 HOST PROXIMITY

Put your host close to your users.

After it, we compressed our assets, so we're shipping fewer bytes over the wire. We are using an efficient protocol, so
that we're not being chatty talking to them, and we made sure our server was big enough for what we were doing.

We should always strive to put our host as close to the user as we can.

So, if we have the host and the user, there's a bunch of hidden things that don't show up in Chrome DevTools, but are
happening under the covers, in terms of how these two things talk to each other over a network. So when the host is
trying to return something, the host exists somewhere, and there's a regional network, like the data center that it's
in, the city that it's in, the ISPs that are a part of there's a bunch of hops that have to happen there. And then once
it's to the main trunks of the Internet, it still has to cross the world, it still has to jump from one part of the
world to another. And then once it's close to the user, it pops back into another regional network and bounces around
to the right city and the right ISP and then the last mile all the way to the user. And so, there's a lot of things
involved.

In the specific case of the example, we're going from Minneapolis to Amsterdam, the time that it takes to do all of
those hops is 117 milliseconds.

That's what a Content Delivery Network is or a CDN, the CDN is the ability for you to put hosts all over the world in
much more regional networks, and those hosts basically make a copy of your content from your original host. After that,
everything can be served from the content delivery network directly, and we no longer have to pay that tax of crossing
around the world.

Bunny CDN is an example of a tool that makes this kind of work.

The CDN did not need to go to Amsterdam to get that cache, it had a copy of this webstie in its edge somewhere in the
United States, somewhere close to where we are right now, that it could return from. We did not have to cross pipes
across the world, this is why CDN's are important.

After all these adjustments, the TTFB is 0.02 seconds, and was 1.3 seconds in the beginning.

So, this is the tactics to improve our TTFB rating, we need to:

1. Compress HTTP Responses;
2. Use efficient protocols, like HTTP 2 or HTTP 3;
3. Set the necessary amount of our Host Capacity;
3. Put our Host close to our users.

In reality, we didn't change any content in the code.



5.2 IMPROVING FCP

The measurement is: How fast your site visibly loads something, but just make sure that your data are telling you that's
a problem for you.

These are the tactics to improve our FCP:

1. Remove Sequence Chains;
2. Preloading Resources;
3. Lazy Load Resources.


5.2.1 REMOVE SEQUENCE CHAINS

First thing, we're gonna collapse our dependencies. Generally, CSS, Fonts and this kind of requests are blocking the
render, with that, they prevent the page from rendering until complete.

The reason is we're using modern CSS, and modern CSS supports import statements, and import lets us say, hey, this CSS
file also uses that other CSS file, go get it. And if we leave and import in the code that we actually ship to the user,
that's creating a dependency, it's creating a sequence. So, we have one CSS file that references another CSS file, that
references another CSS file, and we've created a sequence in our code. With the fonts are happening the same. With the
Javascript files are happening the same, one file points to another file and so on.

So how do we remove this?

You can use some kind of bundler, like Webpack, Rollup or Vite. Rather than letting the browser execute these dependency
between files at runtime, you prepackage them at developer time, at build time using a bundler.

That's the solution: you need to bundle your Javascript or your CSS or whatever into not having import statements in the
final shipped files.

Lightning CSS is an example of a CSS parser, transformer, bundler and minifier.


5.2.2 PRELOAD RESOURCES

Start critical path resources as soon as possible, the central question is: What resources do you absolutely need to
have in order to render the page? How do we get those started as soon as we can?

In the example of the course, the problem is with the request of Google Fonts, where generally are called in some CSS
file, so that CSS file has to come down before those fonts can be loaded.

When you request a Font of Google Font, they try to optimize as little bit, generally with two link statements with
Link preconnect instructions, with that the browser goes ahead and try to start up a connection to those API's before
you know we need them.

This solves a part of the problem, because it like starts up connections, but it doesn't actually tell it to go and get
the files.

Another attribute is the link preload, specifically in the font request, now link preload allows you to basically
instruct the browser to fetch something earlier than it knows about it otherwise. If you know you need something, if you
know something is important, and this is gonna be blocking for your path, you tell the browser to go ahead and start
getting it earlier with Link preload.

So what we can preload? We can preload styles, you can preload scripts, you can preload images, you can preload fonts
and you can even preload a call to fetch.

The limitation is, fonts and fetch requires you to add the crossorigin attribute to that link, which means it's gonna
need the CORS headers, the crossorigin resource sharing headers. You have to be sure that the font or the API that
you're calling, supports your ability to load these things from your website.

Pulling those fonts locally is highly recommended instead make request to Google Fonts servers.


5.2.3 LAZY LOADING RESOURCES

Javascript is Parser Blocking, it prevents parsing content, rendering, and main execution. When the browser encounters
a script tag, it immediately starts downloading it, and as soon as it's done downloading it, it stops what it's doing,
and executes it, right then and there. It prevents parsing any further content, it prevents any further rendering, and
prevents main execution from happening. So, we need to defer it, we need to tell it to execute later.

Instead of defer we can async too, but what's the real difference between them?

They are not exactly the same thing, there's a subtle difference between them, when we decorate our javascript with the
async we are saying to go ahead and start downloading it, whenever you're ready, but as soon as it's done, it will still
block and execute, or block, evaluate, compile, and execute is whenever it's done. It downloads lazy but executes
blocking, it executes as soon as it's ready.

Defer says, go ahead and download this whenever you're ready, but i will not ever start to execute it until just before
DOMContentLoaded. It's gonnae take all the scripts that are deferred, and it's gonna wait until DOMContentLoaded is
about to happen, and then it's going to execute them.

It simple waits for everything else to be done before firing or before executing things right before DOMContentLoaded.

You almost never want async, in most cases you will only use Defer. Multiple Deferred scripts will execute in order
they appear.

If you use type Module will always be deferred, so you don't need to declare the defer in type modules.

About the correct place to put our scripts is in the Head tag or the Body tag it doesn't matter anymore.



5.3 IMPROVING LCP

